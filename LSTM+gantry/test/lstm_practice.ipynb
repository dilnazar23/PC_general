{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "lstm = nn.LSTM(3,3)\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]\n",
    "# this particular inputs batch is one sequence of length 5, with 3 features\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(inputs)\n",
    "print([len(x) for x in inputs])\n",
    "seq_len = max(len(x) for x in inputs)\n",
    "num_features = len(inputs[0][0])\n",
    "print(batch_size,seq_len,num_features)\n",
    "inputs = torch.cat(inputs).view(batch_size, seq_len, num_features)\n",
    "# one input to train on at a time of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, input_path, seq_length=40):\n",
    "        self.inputs,self.targets = torch.load(input_path),\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs) - self.seq_length + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.inputs[idx:idx + self.seq_length], \n",
    "                self.targets[idx + self.seq_length - 1])\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        predictions = self.linear(lstm_out[:, -1, :])\n",
    "        return predictions\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device).float()\n",
    "            targets = targets.to(device).float()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "def train_model(num_hid, optimizer_type, learning_rate, epochs, data_dir):\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Data loading\n",
    "    data_dir = Path(data_dir)\n",
    "    train_dataset = SequenceDataset(\n",
    "        data_dir / \"train_input.pt\",\n",
    "        data_dir / \"train_output.pt\"\n",
    "    )\n",
    "    val_dataset = SequenceDataset(\n",
    "        data_dir / \"val_input.pt\",\n",
    "        data_dir / \"val_output.pt\"\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Model initialization\n",
    "    model = LSTMModel(num_features=4, hidden_size=num_hid).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    if optimizer_type.lower() == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_type.lower() == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer type: {optimizer_type}\")\n",
    "    \n",
    "    # TensorBoard setup\n",
    "    writer = SummaryWriter('runs/lstm_training')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device).float()\n",
    "            targets = targets.to(device).float()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Validation every 100 epochs\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            val_loss = validate_model(model, val_loader, criterion, device)\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], '\n",
    "                  f'Training Loss: {avg_train_loss:.4f}, '\n",
    "                  f'Validation Loss: {val_loss:.4f}')\n",
    "            \n",
    "            # Log to TensorBoard\n",
    "            writer.add_scalar('Training Loss', avg_train_loss, epoch)\n",
    "            writer.add_scalar('Validation Loss', val_loss, epoch)\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'lstm_model.pth')\n",
    "    writer.close()\n",
    "\n",
    "def main_test():\n",
    "    parser = argparse.ArgumentParser(description='Train LSTM model')\n",
    "    parser.add_argument('--num_hid', type=int, default=64,\n",
    "                      help='number of hidden units')\n",
    "    parser.add_argument('--optimizer', type=str, default='adam',\n",
    "                      help='optimizer type (adam or sgd)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001,\n",
    "                      help='learning rate')\n",
    "    parser.add_argument('--epochs', type=int, default=1000,\n",
    "                      help='number of epochs')\n",
    "    parser.add_argument('--data_dir', type=str, default='./data',\n",
    "                      help='directory containing the data files')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    train_model(\n",
    "        num_hid=args.num_hid,\n",
    "        optimizer_type=args.optimizer,\n",
    "        learning_rate=args.lr,\n",
    "        epochs=args.epochs,\n",
    "        data_dir=args.data_dir\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    num_hid=10,\n",
    "    optimizer_type=\"adam\",\n",
    "    learning_rate=0.01,\n",
    "    epochs=100,\n",
    "    data_dir='../data'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        predictions = self.linear(lstm_out[:, -1, :])\n",
    "        return predictions\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMModel(num_features=32, hidden_size=64)\n",
    "model.load_state_dict(torch.load('../1257_1802.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create dummy input tensor\n",
    "dummy_input = torch.randn([64, 40, 32])\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = \"../lstm_1257_1802.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,               # model being run\n",
    "    dummy_input,        # model input (or a tuple for multiple inputs)\n",
    "    onnx_path,          # where to save the model\n",
    "    export_params=True, # store the trained parameter weights inside the model file\n",
    "    opset_version=11,   # the ONNX version to export the model to\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    input_names=['input'],     # the model's input names\n",
    "    output_names=['output'],   # the model's output names\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},  # variable length axes\n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.norm = nn.LayerNorm(num_features)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        predictions = self.linear(lstm_out[:, -1, :])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to onnx\n",
    "# import pytorch model\n",
    "import onnx\n",
    "model = LSTMModel(num_features=32, hidden_size=32)\n",
    "model_path= '../lstm_03-04_11:18.pth'\n",
    "state_dict = torch.load(torch.load(model_path))\n",
    "# If the state_dict is wrapped (e.g., by DataParallel), use the following line\n",
    "for item in state_dict:\n",
    "    print(item)\n",
    "state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "print(f\"Model loaded successfully from {model_path}\")\n",
    "model.eval()\n",
    "input_shape = (1, 40, 32)\n",
    "dummy_input = torch.randn(input_shape)\n",
    "\n",
    "# Export the model to ONNX\n",
    "try:\n",
    "    torch.onnx.export(\n",
    "        model,               # Model being exported\n",
    "        dummy_input,         # Dummy input\n",
    "        onnx_path,           # Output file path\n",
    "        export_params=True,  # Store the trained parameter weights inside the model file\n",
    "        opset_version=11,    # ONNX version to export the model to\n",
    "        do_constant_folding=True,  # Optimize the model by executing constant folding\n",
    "        input_names=['input'],      # Name of the input\n",
    "        output_names=['output'],    # Name of the output\n",
    "        dynamic_axes={'input': {0: 'batch_size'},  # Variable batch size\n",
    "                        'output': {0: 'batch_size'}}\n",
    "    )\n",
    "    print(f\"Model exported successfully to {onnx_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error exporting model to ONNX: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiemtn with dataset style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, train_path, seq_length=40):\n",
    "        self.inputs, self.targets = torch.load(train_path)\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs) - self.seq_length + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.inputs[idx:idx + self.seq_length], \n",
    "                self.targets[idx + self.seq_length - 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '0227'\n",
    "for file in Path('../data/processed_data').glob(f'{data_dir}*.pt'):\n",
    "    print(file)\n",
    "    ## add val_dataset here\n",
    "    train_dataset = SequenceDataset(file)\n",
    "    for item in enumerate(train_dataset.targets):\n",
    "        print(f'target:{item}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '0227'\n",
    "for file in Path('../data/processed_data').glob(f'{data_dir}*.pt'):\n",
    "    print(file)\n",
    "    ## add val_dataset here\n",
    "    train_dataset = SequenceDataset(file)\n",
    "    for item in enumerate(train_dataset.inputs):\n",
    "        print(f'input shape:{item[1].shape}')\n",
    "        print(f'input:{item}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '0227'\n",
    "for file in Path('../data/processed_data').glob(f'{data_dir}*.pt'):\n",
    "    print(file)\n",
    "    ## add val_dataset here\n",
    "    train_dataset = SequenceDataset(file)\n",
    "    for item in enumerate(train_dataset):\n",
    "        print(f'input shape:{item[1][0].shape}')\n",
    "        print(f'input:{item}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training test with differnet  data set style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, train_path, seq_length=40):\n",
    "        self.inputs, self.targets = torch.load(train_path)\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs) - self.seq_length + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= self.__len__():\n",
    "            raise IndexError(f\"Index {idx} out of bounds\")\n",
    "        return (self.inputs[idx:idx + self.seq_length], \n",
    "                self.targets[idx:idx + self.seq_length])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '0227_change'\n",
    "for file in Path('../data/processed_data').glob(f'{data_dir}*.pt'):\n",
    "    print(file)\n",
    "    ## add val_dataset here\n",
    "    train_dataset = SequenceDataset(file)\n",
    "    print(train_dataset.targets.shape,train_dataset.inputs.shape)\n",
    "    print(len(train_dataset))\n",
    "    # for item in enumerate(train_dataset):\n",
    "    #     print(f'set:{item}')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.norm = nn.LayerNorm(num_features)\n",
    "        # self.hid = nn.Linear(num_features, 16)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        # hid = self.hid(F.leaky_relu(norm))\n",
    "        # # lstm_out, _ = self.lstm(x)\n",
    "        # print(f'lstm output shape: {lstm_out.shape}')\n",
    "        # print(f'lstm output values: {lstm_out[:, -1, :].shape}')\n",
    "        # h_0 = Variable(torch.zeros(1, 12645, 32).cuda())\n",
    "        # c_0 = Variable(torch.zeros(1, 12645, 32).cuda())\n",
    "        lstm_out,_ = self.lstm(x)\n",
    "        return self.linear(lstm_out) \n",
    "\n",
    "        # predictions = self.linear(lstm_out[:, -1, :])\n",
    "        # return lstm_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params:\n",
    "num_hid = 32\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "optimizer_type = 'adam'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = len(train_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True,pin_memory=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model initialization\n",
    "model = LSTMModel(num_features=32, hidden_size=num_hid).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "if optimizer_type.lower() == 'adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "elif optimizer_type.lower() == 'sgd':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported optimizer type: {optimizer_type}\")\n",
    "\n",
    "# TensorBoard setup\n",
    "# writer = SummaryWriter('runs/lstm_training')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs) \n",
    "        # print(outputs.shape)       \n",
    "        loss = criterion(outputs, targets).to(device)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        #print(loss.item())\n",
    "    # Calculate average training loss\n",
    "    avg_train_loss = total_loss\n",
    "    \n",
    "    # Validation every 100 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "    #     val_loss = validate_model(model, val_loader, criterion, device)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}]',\n",
    "                f'Training Loss: {avg_train_loss:.4f}')\n",
    "    if epoch == epochs-1:\n",
    "        print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((324-315)**2+(443-437)**2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(outputs[-1]-targets[-1])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device).float()\n",
    "            targets = targets.to(device).float()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(model, device='cuda', input_shape=(40,1,32)):\n",
    "    \n",
    "    curr_time = datetime.now().strftime(\"%m-%d_%H:%M\")\n",
    "    save_path = f\"lstm_{curr_time}\"\n",
    "    # Save PyTorch model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_architecture': model.__class__.__name__\n",
    "    }, f\"{save_path}.pth\")\n",
    "    \n",
    "    # Prepare model for ONNX export\n",
    "    model.eval()    \n",
    "    # Create dummy input tensor\n",
    "    dummy_input = torch.randn(input_shape, device=device)\n",
    "    \n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        f\"{save_path}.onnx\",\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={'input' : {0 : 'sequence_length'},    # variable length axes\n",
    "                    'output' : {0 : 'sequence_length'}}\n",
    "    )\n",
    "    print(f\"Model saved as {save_path}.pth and {save_path}.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/processed_data/0227_change_DMF_train_set.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_338260/1713998515.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.inputs, self.targets = torch.load(train_path)\n",
      "/tmp/ipykernel_338260/2092536470.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('lstm_03-10_16:14.pth')['model_state_dict'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300] Training Loss: 11.8856\n",
      "Epoch [20/300] Training Loss: 5.7231\n",
      "Epoch [30/300] Training Loss: 5.0555\n",
      "Epoch [40/300] Training Loss: 4.5847\n",
      "Epoch [50/300] Training Loss: 4.2662\n",
      "Epoch [60/300] Training Loss: 3.9925\n",
      "Epoch [70/300] Training Loss: 3.7667\n",
      "Epoch [80/300] Training Loss: 3.5040\n",
      "Epoch [90/300] Training Loss: 3.3741\n",
      "Epoch [100/300] Training Loss: 3.2416\n",
      "Epoch [100/300] Validation Loss: 2.7751\n",
      "Epoch [110/300] Training Loss: 3.0930\n",
      "Epoch [120/300] Training Loss: 2.8657\n",
      "Epoch [130/300] Training Loss: 2.7629\n",
      "Epoch [140/300] Training Loss: 2.9207\n",
      "Epoch [150/300] Training Loss: 2.7572\n",
      "Epoch [160/300] Training Loss: 2.5234\n",
      "Epoch [170/300] Training Loss: 2.4317\n",
      "Epoch [180/300] Training Loss: 2.3397\n",
      "Epoch [190/300] Training Loss: 3.5743\n",
      "Epoch [200/300] Training Loss: 3.1331\n",
      "Epoch [200/300] Validation Loss: 2.2262\n",
      "Epoch [210/300] Training Loss: 2.7379\n",
      "Epoch [220/300] Training Loss: 2.5580\n",
      "Epoch [230/300] Training Loss: 2.4433\n",
      "Epoch [240/300] Training Loss: 2.3360\n",
      "Epoch [250/300] Training Loss: 2.2708\n",
      "Epoch [260/300] Training Loss: 2.2593\n",
      "Epoch [270/300] Training Loss: 2.2140\n",
      "Epoch [280/300] Training Loss: 2.1774\n",
      "Epoch [290/300] Training Loss: 2.1746\n",
      "Epoch [300/300] Training Loss: 2.1740\n",
      "Epoch [300/300] Validation Loss: 1.8365\n",
      "tensor(14.3534, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
      "Model saved as lstm_03-10_16:56.pth and lstm_03-10_16:56.onnx\n"
     ]
    }
   ],
   "source": [
    "#prepare data\n",
    "data_dir = '0227_change'\n",
    "train_dataset = None\n",
    "for file in Path('../data/processed_data').glob(f'{data_dir}*.pt'):\n",
    "    print(file)\n",
    "    train_dataset = SequenceDataset(file)\n",
    "val_dataset = SequenceDataset('../data/processed_data/0227_val_change_DMF_set.pt')\n",
    "\n",
    "# params:\n",
    "num_hid = 32\n",
    "learning_rate = 0.15\n",
    "epochs = 300\n",
    "optimizer_type = 'adam'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = len(train_dataset)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True,pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model initialization\n",
    "model = LSTMModel(num_features=32, hidden_size=num_hid).to(device)\n",
    "model.load_state_dict(torch.load('lstm_03-10_16:14.pth')['model_state_dict'])\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "if optimizer_type.lower() == 'adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "elif optimizer_type.lower() == 'sgd':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported optimizer type: {optimizer_type}\")\n",
    "\n",
    "# TensorBoard setup\n",
    "# writer = SummaryWriter('runs/lstm_training')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs) \n",
    "        # print(outputs.shape)       \n",
    "        loss = criterion(outputs, targets).to(device)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        #print(loss.item())\n",
    "    # Calculate average training loss\n",
    "    avg_train_loss = total_loss\n",
    "    \n",
    "    # Validation every 100 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "    #     val_loss = validate_model(model, val_loader, criterion, device)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}]',\n",
    "                f'Training Loss: {avg_train_loss:.4f}')\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        val_loss = validate_model(model, val_loader, criterion, device)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}]',\n",
    "                f'Validation Loss: {val_loss:.4f}')\n",
    "    if epoch == epochs-1:\n",
    "        print(max(abs((outputs-targets)).reshape(-1)))\n",
    "        export_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(42.4153, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sum_err = 0\n",
    "for x in abs((outputs-targets)).reshape(-1):\n",
    "    sum_err += x\n",
    "print(sum_err/(len(outputs)*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
