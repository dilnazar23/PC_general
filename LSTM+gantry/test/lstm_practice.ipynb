{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.5525,  0.6355, -0.3968]]),\n",
       " tensor([[-0.6571, -1.6428,  0.9803]]),\n",
       " tensor([[-0.0421, -0.8206,  0.3133]]),\n",
       " tensor([[-1.1352,  0.3773, -0.2824]]),\n",
       " tensor([[-2.5667, -1.4303,  0.5009]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "lstm = nn.LSTM(3,3)\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]\n",
    "# this particular inputs batch is one sequence of length 5, with 3 features\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1]\n",
      "5 1 3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m num_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch_size,seq_len,num_features)\n\u001b[0;32m----> 6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(batch_size, seq_len, num_features)\n",
      "\u001b[0;31mTypeError\u001b[0m: cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "batch_size = len(inputs)\n",
    "print([len(x) for x in inputs])\n",
    "seq_len = max(len(x) for x in inputs)\n",
    "num_features = len(inputs[0][0])\n",
    "print(batch_size,seq_len,num_features)\n",
    "inputs = torch.cat(inputs).view(batch_size, seq_len, num_features)\n",
    "# one input to train on at a time of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, input_path, seq_length=40):\n",
    "        self.inputs,self.targets = torch.load(input_path),\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs) - self.seq_length + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.inputs[idx:idx + self.seq_length], \n",
    "                self.targets[idx + self.seq_length - 1])\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        predictions = self.linear(lstm_out[:, -1, :])\n",
    "        return predictions\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device).float()\n",
    "            targets = targets.to(device).float()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "def train_model(num_hid, optimizer_type, learning_rate, epochs, data_dir):\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Data loading\n",
    "    data_dir = Path(data_dir)\n",
    "    train_dataset = SequenceDataset(\n",
    "        data_dir / \"train_input.pt\",\n",
    "        data_dir / \"train_output.pt\"\n",
    "    )\n",
    "    val_dataset = SequenceDataset(\n",
    "        data_dir / \"val_input.pt\",\n",
    "        data_dir / \"val_output.pt\"\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Model initialization\n",
    "    model = LSTMModel(num_features=4, hidden_size=num_hid).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    if optimizer_type.lower() == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_type.lower() == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer type: {optimizer_type}\")\n",
    "    \n",
    "    # TensorBoard setup\n",
    "    writer = SummaryWriter('runs/lstm_training')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device).float()\n",
    "            targets = targets.to(device).float()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Validation every 100 epochs\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            val_loss = validate_model(model, val_loader, criterion, device)\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], '\n",
    "                  f'Training Loss: {avg_train_loss:.4f}, '\n",
    "                  f'Validation Loss: {val_loss:.4f}')\n",
    "            \n",
    "            # Log to TensorBoard\n",
    "            writer.add_scalar('Training Loss', avg_train_loss, epoch)\n",
    "            writer.add_scalar('Validation Loss', val_loss, epoch)\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'lstm_model.pth')\n",
    "    writer.close()\n",
    "\n",
    "def main_test():\n",
    "    parser = argparse.ArgumentParser(description='Train LSTM model')\n",
    "    parser.add_argument('--num_hid', type=int, default=64,\n",
    "                      help='number of hidden units')\n",
    "    parser.add_argument('--optimizer', type=str, default='adam',\n",
    "                      help='optimizer type (adam or sgd)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001,\n",
    "                      help='learning rate')\n",
    "    parser.add_argument('--epochs', type=int, default=1000,\n",
    "                      help='number of epochs')\n",
    "    parser.add_argument('--data_dir', type=str, default='./data',\n",
    "                      help='directory containing the data files')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    train_model(\n",
    "        num_hid=args.num_hid,\n",
    "        optimizer_type=args.optimizer,\n",
    "        learning_rate=args.lr,\n",
    "        epochs=args.epochs,\n",
    "        data_dir=args.data_dir\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8171/793031045.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.inputs,self.targets = torch.load(input_path),\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_hid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madam\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 56\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(num_hid, optimizer_type, learning_rate, epochs, data_dir)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Data loading\u001b[39;00m\n\u001b[1;32m     55\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m Path(data_dir)\n\u001b[0;32m---> 56\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSequenceDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_input.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_output.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     59\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m SequenceDataset(\n\u001b[1;32m     61\u001b[0m     data_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_input.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     62\u001b[0m     data_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_output.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     65\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m, in \u001b[0;36mSequenceDataset.__init__\u001b[0;34m(self, input_path, target_path, seq_length)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_path, target_path, seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(input_path),\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_length \u001b[38;5;241m=\u001b[39m seq_length\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    num_hid=10,\n",
    "    optimizer_type=\"adam\",\n",
    "    learning_rate=0.01,\n",
    "    epochs=100,\n",
    "    data_dir='../data'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14167/959342144.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('../1257_1802.pth'))\n",
      "/home/aquila-nazar/observer_dev/dnn_env/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4279: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        predictions = self.linear(lstm_out[:, -1, :])\n",
    "        return predictions\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMModel(num_features=32, hidden_size=64)\n",
    "model.load_state_dict(torch.load('../1257_1802.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create dummy input tensor\n",
    "dummy_input = torch.randn([64, 40, 32])\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = \"../lstm_1257_1802.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,               # model being run\n",
    "    dummy_input,        # model input (or a tuple for multiple inputs)\n",
    "    onnx_path,          # where to save the model\n",
    "    export_params=True, # store the trained parameter weights inside the model file\n",
    "    opset_version=11,   # the ONNX version to export the model to\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    input_names=['input'],     # the model's input names\n",
    "    output_names=['output'],   # the model's output names\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},  # variable length axes\n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
